{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMb/nZZCFx0jVfJPg1MNfPE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pdh93621/Deep-learning/blob/main/IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNBBvTBUzoiJ"
      },
      "source": [
        "## IMDB리뷰 토큰화하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPvBxIZLzu1g"
      },
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrsVrWNrzzxF"
      },
      "source": [
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNRpq50fz2jV"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R0ipYddz3gR",
        "outputId": "5e00b62a-0d29-4503-997d-9caaf8e2f0a0"
      },
      "source": [
        "# 어떤 리뷰 가져오기\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x7fb6df3a8250>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJlFZ4-rz_-e"
      },
      "source": [
        "train_df = pd.read_csv('IMDb_Reviews.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNAxFu-t0Gx2",
        "outputId": "dc3a2736-dcbb-4d5b-a892-7fc0f2262583"
      },
      "source": [
        "train_df['review']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        My family and I normally do not watch local mo...\n",
              "1        Believe it or not, this was at one time the wo...\n",
              "2        After some internet surfing, I found the \"Home...\n",
              "3        One of the most unheralded great works of anim...\n",
              "4        It was the Sixties, and anyone with long hair ...\n",
              "                               ...                        \n",
              "49995    the people who came up with this are SICK AND ...\n",
              "49996    The script is so so laughable... this in turn,...\n",
              "49997    \"So there's this bride, you see, and she gets ...\n",
              "49998    Your mind will not be satisfied by this nobud...\n",
              "49999    The chaser's war on everything is a weekly sho...\n",
              "Name: review, Length: 50000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-wSkc4h0KiB",
        "outputId": "32de7f31-b0d8-4ac5-ce80-79c3a4a2c253"
      },
      "source": [
        "# sentiment는 뭔지 도저히 모르겠다\n",
        "for i in train_df:\n",
        "  print(i,train_df[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review 0        My family and I normally do not watch local mo...\n",
            "1        Believe it or not, this was at one time the wo...\n",
            "2        After some internet surfing, I found the \"Home...\n",
            "3        One of the most unheralded great works of anim...\n",
            "4        It was the Sixties, and anyone with long hair ...\n",
            "                               ...                        \n",
            "49995    the people who came up with this are SICK AND ...\n",
            "49996    The script is so so laughable... this in turn,...\n",
            "49997    \"So there's this bride, you see, and she gets ...\n",
            "49998    Your mind will not be satisfied by this nobud...\n",
            "49999    The chaser's war on everything is a weekly sho...\n",
            "Name: review, Length: 50000, dtype: object\n",
            "sentiment 0        1\n",
            "1        0\n",
            "2        0\n",
            "3        1\n",
            "4        0\n",
            "        ..\n",
            "49995    0\n",
            "49996    0\n",
            "49997    0\n",
            "49998    0\n",
            "49999    1\n",
            "Name: sentiment, Length: 50000, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1cn0Lac0U_G"
      },
      "source": [
        "# 토큰화(최대 2**13 = 8192), 이때 토큰화가 전혀 되지 않아도 기본적으로 256개의 데이터를 깔고 들어간다\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_df['review'], target_vocab_size= 2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQz_LVYh03te",
        "outputId": "cc1da57b-32b3-4d19-eb55-690616ba2898"
      },
      "source": [
        "print(tokenizer.subwords[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 't_', 'as_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', 'are_', ' (', 'have_', 'his_', 'film_', 'not_', 'be_', 'you_', 'ing_', ' \"', 'ed_', 'it', 'd_', 'an_', 'at_', 'by_', 'he_', 'one_', 'who_', 'from_', 'y_', 'or_', 'e_', 'like_', 'all_', '\" ', 'they_', 'so_', 'just_', 'has_', ') ', 'about_', 'her_', 'out_', 'This_', 'some_', 'movie', 'ly_', 'film', 'very_', 'more_', 'It_', 'what_', 'would_', 'when_', 'if_', 'good_', 'up_', 'which_', 'their_', 'only_', 'even_', 'my_', 'really_', 'had_', 'can_', 'no_', 'were_', 'see_', '? ', 'she_', 'than_', '! ', 'there_', 'been_', 'get_', 'into_', 'will_', ' - ', 'much_', 'n_', 'because_', 'ing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaYAPggt2PVu",
        "outputId": "f05079d2-a421-4601-f095-a58cd060da80"
      },
      "source": [
        "print(train_df['review'][20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itcAErIy2WFl",
        "outputId": "df280dc8-2cde-41f1-db77-545c75418412"
      },
      "source": [
        "print('토크화된 샘플 질문 : {}'.format(tokenizer.encode(train_df['review'][20])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "토크화된 샘플 질문 : [1590, 4162, 132, 7107, 1892, 2983, 578, 76, 12, 4632, 3422, 7, 160, 175, 372, 2, 5, 39, 8051, 8, 84, 2652, 497, 39, 8051, 8, 1374, 5, 3461, 2012, 48, 5, 2263, 21, 4, 2992, 127, 4729, 711, 3, 1391, 8044, 3557, 1277, 8102, 2154, 5681, 9, 42, 15, 372, 2, 3773, 4, 3502, 2308, 467, 4890, 1503, 11, 3347, 1419, 8127, 29, 5539, 98, 6099, 58, 94, 4, 1388, 4230, 8057, 213, 3, 1966, 2, 1, 6700, 8044, 9, 7069, 716, 8057, 6600, 2, 4102, 36, 78, 6, 4, 1865, 40, 5, 3502, 1043, 1645, 8044, 1000, 1813, 23, 1, 105, 1128, 3, 156, 15, 85, 33, 23, 8102, 2154, 5681, 5, 6099, 8051, 8, 7271, 1055, 2, 534, 22, 1, 3046, 5214, 810, 634, 8120, 2, 14, 71, 34, 436, 3311, 5447, 783, 3, 6099, 2, 46, 71, 193, 25, 7, 428, 2274, 2260, 6487, 8051, 8, 2149, 23, 1138, 4117, 6023, 163, 11, 148, 735, 2, 164, 4, 5277, 921, 3395, 1262, 37, 639, 1349, 349, 5, 2460, 328, 15, 5349, 8127, 24, 10, 16, 10, 17, 8054, 8061, 8059, 8062, 29, 6, 6607, 8126, 8053]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ejtb1g42rSJ",
        "outputId": "a3e72bcb-4c32-416a-976c-3ae436561e2a"
      },
      "source": [
        "sample_str = \"It's mind-blowing to me that this film was even made.\"\n",
        "\n",
        "tokenized_string = tokenizer.encode(sample_str)\n",
        "print('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print('기존 문장: {}'.format(original_string))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정수 인코딩 후의 문장 [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 79, 681, 8058]\n",
            "기존 문장: It's mind-blowing to me that this film was even made.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1WyDMcA3s0x",
        "outputId": "21de2cb9-5add-45fe-b3f7-1004441e6585"
      },
      "source": [
        "print('단어 집합의 크기(Vocab size) : ', tokenizer.vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기(Vocab size) :  8268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1qQlGPe4s-a",
        "outputId": "16a0febe-36f9-4e0b-8135-6a44b9793a6c"
      },
      "source": [
        "# 각 index별 문자열 출력\n",
        "for ts in tokenized_string:\n",
        "  print(f'{ts} ---> {tokenizer.decode([ts])}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "137 ---> It\n",
            "8051 ---> '\n",
            "8 ---> s \n",
            "910 ---> mind\n",
            "8057 ---> -\n",
            "2169 ---> blow\n",
            "36 ---> ing \n",
            "7 ---> to \n",
            "103 ---> me \n",
            "13 ---> that \n",
            "14 ---> this \n",
            "32 ---> film \n",
            "18 ---> was \n",
            "79 ---> even \n",
            "681 ---> made\n",
            "8058 ---> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjnHU70L5Qm0"
      },
      "source": [
        "sample_str2 = \"It's mind-blowing to me that this film was evenxyz made.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvxaXWRi6Q-6",
        "outputId": "6843f5ea-1d78-4853-9d24-4b97642e682f"
      },
      "source": [
        "tokenized_string2 = tokenizer.encode(sample_str2)\n",
        "print(f'정수 인코딩 후의 문장 : {tokenized_string2}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정수 인코딩 후의 문장 : [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 7974, 8132, 8133, 997, 681, 8058]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I7s-wNp6hse",
        "outputId": "8d7727be-60c7-4071-f985-755a806a72a9"
      },
      "source": [
        "original_string2 = tokenizer.decode(tokenized_string2)\n",
        "print(f\"기존 문장 : {original_string2}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "기존 문장 : It's mind-blowing to me that this film was evenxyz made.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPPUc99k6rmw",
        "outputId": "475763e8-ccaa-4803-cd31-e68f80df0c5c"
      },
      "source": [
        "# 만일 evenxyz 같이 의미없는 단어가 나오면 토크화된 것 중 가장 큰 덩어리로 자르고 나머지는 단일 문자로 처리하는듯\n",
        "for ts in tokenized_string2:\n",
        "  print(f'{ts} ---> {tokenizer.decode([ts])}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "137 ---> It\n",
            "8051 ---> '\n",
            "8 ---> s \n",
            "910 ---> mind\n",
            "8057 ---> -\n",
            "2169 ---> blow\n",
            "36 ---> ing \n",
            "7 ---> to \n",
            "103 ---> me \n",
            "13 ---> that \n",
            "14 ---> this \n",
            "32 ---> film \n",
            "18 ---> was \n",
            "7974 ---> even\n",
            "8132 ---> x\n",
            "8133 ---> y\n",
            "997 ---> z \n",
            "681 ---> made\n",
            "8058 ---> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQe60ETV6xET",
        "outputId": "fe12a512-1e59-4d87-d678-c5c72c3abb02"
      },
      "source": [
        "sample_str3 = \"It's mind-blowing to me that this film was even made.\"\n",
        "\n",
        "tokenized_string3 = tokenizer.encode(sample_str3)\n",
        "print('정수 인코딩 후의 문장 {}'.format(tokenized_string3))\n",
        "\n",
        "original_string3 = tokenizer.decode(tokenized_string3)\n",
        "print('기존 문장: {}'.format(original_string3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정수 인코딩 후의 문장 [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 79, 681, 8058]\n",
            "기존 문장: It's mind-blowing to me that this film was even made.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI1eFUMJ9P9W",
        "outputId": "e2f8a166-1783-41a2-cf22-1516fca53097"
      },
      "source": [
        "# 동일 문장을 다시 실행해도 동일한 결과를 얻는다\n",
        "for ts in tokenized_string3:\n",
        "  print(f'{ts} ---> {tokenizer.decode([ts])}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "137 ---> It\n",
            "8051 ---> '\n",
            "8 ---> s \n",
            "910 ---> mind\n",
            "8057 ---> -\n",
            "2169 ---> blow\n",
            "36 ---> ing \n",
            "7 ---> to \n",
            "103 ---> me \n",
            "13 ---> that \n",
            "14 ---> this \n",
            "32 ---> film \n",
            "18 ---> was \n",
            "79 ---> even \n",
            "681 ---> made\n",
            "8058 ---> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "K4wu9hbR9Wj9",
        "outputId": "692e1380-4b7b-4e16-9868-8d675dba026b"
      },
      "source": [
        "# 토큰화된 데이터에 각 문자열은 존재하지 않지만 기존의 default 값 256개에 기본적인 문자들은 저장되어 있는듯 하다\n",
        "if 'l' in tokenizer.subwords:\n",
        "  print('Yes')\n",
        "else:\n",
        "  print('No')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dbf868828fce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;34m'i'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Yes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSHTG3IG-ExE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}